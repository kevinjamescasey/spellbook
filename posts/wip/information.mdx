

The definitions I've found for "information" (and data) vary greatly across different sources and context. There doesn't seem to be any clear scientific consensus on what it is.

In the colloquial use we just talk about information as something we can gain access to in order to inform ourselves about something. It is something recorded or transfered that we can percieve in order to learn about something else. So it is always information about something else and is presumably useful in some way even if just for fleetinge amusement. It requires a human to percieve it and make some sense out of it. We, the mainstream masses, use the term this way often in the realm of Information Technlogy and there is little motivation to delve into an attempt at providing further rigor.

In (Information Theory)[https://en.wikipedia.org/wiki/Information_theory] there doesn't seem to be any focus on describing what information is, but rather on measuring the amount of uncertainty present in a physical system by counting the number of possible states. I guess it assumed that we can learn more from a system with a higher number of possible state configurations and it requires more communication to fully describe (convey the information) of a more system with more possibilities. In practice we only pay attention to state configurations we care about, for example, which side of a coin is up rather than the atomic conent of the coin. This seems somewhat related to the colliquial use of "information" in that it relies our our perception of what information is. If the information we care about is which side of single coin is up then it said the complete state of the system can be conveyed in a single bit. If the system state we care about is which side of several coins are up then several bits of information would have to be conveyed for us to learn the complete state of the system. Because these examples use the simple binary state of coins it would seem that 'bit' defined in Information Theory is the same unit of measurement as is used in Information Technology such as electronic storage media and communication networks that use binary signals to store and convey information. It is simlar to representing information with binary digits where each digit has one of the two possible states, 0 or 1. But the amount of information conveyed or present in the system doesn't have to be expressed with binary numbers. [A number system with a different base can be used](https://en.wikipedia.org/wiki/Units_of_information#Primary_units).

### Physics Definition

+ particular configuration of matter in a space out of all possibilities?
  - is this information or is it state and information is a measure of how many unique states are possible?
+ computer science is a subset of Physics
+ software engineering is physical
+ It might not be a good idea to try to unify the concepts of information in Physics, Information Theory, and Information Technlogy
  - They might just be different definitions in different realms of thought and study
  - It might be fun, inspiring, or fruitful in some way


### Informaton Technology

+ all meta-information
  - info stored as an abstract representation of other info
+ keeping track of things we think are immportant
  - money, materials, product inventory, assets
    - how much and where
  - recorded captured events
    - measurements, audio, video, transcription
+ IT before computers
  - cave paint, stone tablets, printing press
+ why do we do it?
+ how can we use it to improve wellbeing?
 - planning
 - learning beyond what we can learn with our brain memory alone
 
#### Location, Locaiton, Location

+ every data structure stores info in a way to make certain operations quick
  - lookup/retrieval, accretion, removal
+ databases are data structures
  - generally persisted to disk but not always
+ redplanetlabs Rama pstates
  - https://redplanetlabs.com/docs/~/pstates.html#_pstates_versus_databases


### Information Theory

Information Theory from 

+ rate of transfer
+ info gain (learning)
+ info loss
+ entropy
  - amount of surprise or uncertainty
  - amount of information conveyed by identifying the outcome of a trial
  - die roll unveils more certainty than a coin flip because each possible state in a die roll is less certain than those in a coin flip
    - lower probablity events convey more information when they occur
+ Does state of some physical system have to be conveyed for information to exist or does that state itself contain or consist of information regardless of wether it is being transfered or percieved?

### Comparison of Loss of Information
This seems related to measuring the amount of information as is done in Information Theorye

- compression algroithms
 - zip
 - jpeg
- dimensionality reduction for Machine Learning (ML)
  - [Latent Semantic Analysis (LSA)](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
- The "learning" part of ML is recording some representation of sambles observed but not necessarily keeping all information


### Computers
+ theory
  - Turing machines
+ essence of real machines
  - digital
  - algorithms move bits around in locations
    - or do they copy it via signal and delete the location
  - Rich Hickey's PLOP
  - transforming
    - a composite structure
      - moving, deleting, adding to
    - primitive type
      - arithmetic
        - hardwired algorithm 

### Utility

Information does not have to be useful in any way to have its amount measured.
42 bits of useless noise and 42 bits of natural language sent between humans are both 42 bits.

To make utility out of information you have to define the configurations of state that you care about and stick to that definition. For engineering and utility this is useful, but the definition you choose has no fundamental meaning outside the context in which you use that definition. We have shared useful conventions in computers that we've used for years. It doesn't mean any of them are fundamental truths or physical principals. They are just fundamental to our current technology.

Is the amount of information in a space dependent on how many spaces you can divide it into logically? How many you'll be able to detect matter in?
Physicists say there is a maximum information density due to the formation of black holes when more than a certain amount of matter is present in a confined area of space.


### Google AI answer

According to current scientific understanding, information itself does not exist without matter, as information is always encoded within a physical system, like a pattern of bits on a computer chip or the arrangement of atoms in a molecule; therefore, it requires a physical medium to be stored and transmitted, making it dependent on matter to exist. 
Key points to consider:
Information is not a substance:
Unlike matter, information is not a tangible entity with mass, but rather a concept describing the arrangement or pattern of something. 
Physical representation is necessary:
To convey information, it must be encoded within a physical system, like a written word on paper, a digital signal, or even the structure of a brain. 
Philosophical interpretations:
Some physicists, like John Wheeler, propose the idea of "It from Bit," suggesting that information might be more fundamental than matter itself, but this remains a theoretical concept. 

